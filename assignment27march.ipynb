{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f10aaf6a-e2a2-49af-bab2-3340fc8932c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it\n",
    "# represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bfaefd1-d771-416b-bbd8-8a84f4b671e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R-squared in linear regression models is used to define how much the model is accurate and help to check the regression model\n",
    "# accuracy. It is calculated as sqrt=1-(SSres/SStotal)\n",
    "# where SSres = sum of squared residual (error)\n",
    "#       SStotal= sum of squared total \n",
    "# SSres=∑(yi-ŷi)^2\n",
    "# SStotal=∑(yi-ȳ)^2\n",
    "# it tell us quality of model's \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28d3f40a-7b4f-4557-b8c6-840188b7ab12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. Define adjusted R-squared and explain how it differs from the regular R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c58dbe70-de9b-4626-bf6a-47818699f8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # R-squared is tell us accuracy of model and when we add any new feature such that output feature depend directly then Adjusted \n",
    "# R- Squared increase as well R - squared increase but R- Squared > Adjust R- squared always, let if we add another feature in \n",
    "# dataset such that oputput feature is not much depend on that feature then adjust R -squared decrease but R-squared increase small .\n",
    "# Adjust R-squared calculated as = 1-((1-R^2)(N-1)/(N-P-1))\n",
    "# where R^2 = R-squared which we calculated above \n",
    "# N= sample size\n",
    "# P=Number of independent feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09031494-c1a3-420e-bb9a-1506585f3436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. When is it more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a87e6ae-6117-4ebc-88d2-4fb68b882682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted R-squared is more appropriate to use when comparing regression models with a different number of independent variables \n",
    "# or predictors. It addresses a limitation of the regular R-squared, \n",
    "# which tends to increase with the addition of more predictors, even if they do not contribute significantly to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b12d431a-a156-48f0-8821-1a75f08498cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
    "# calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "372927a5-4103-4141-bed9-b27792e4fe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE ( Root mean squared error ) it is used  for  minimize the error, calculated as\n",
    "# √(Σ(y_pred-y_actual)^2)/n\n",
    "# MSE ( Mean squared error) it is used for minimize the error, calculated as \n",
    "# √(Σ(y_pred-y_actual)^2)/n\n",
    "# MAE ( Mean absolute eroor) it is used for minimze the error, calculted as \n",
    "# (Σ|y_pred-y_actual|)/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed25b2da-102e-4196-a0ad-f13f79c31b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
    "# regression analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b7e6f6f-c4b8-413d-bf9e-4d9677abfad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advantages                                Disadvantages \n",
    "# MSE\n",
    "# 1) It is differentail equation           1) not  robust to outlier \n",
    "# 2) having one global minima              2)It is  not in the same unit \n",
    "# 3) Quadratic equation \n",
    "\n",
    "# RMSE:\n",
    "# 1) It is differentail equation            1) not  robuts to outlier \n",
    "# 2) Having one global minima \n",
    "# 3) It is in the same unit \n",
    "\n",
    "# MAE:\n",
    "# 1) It is in the same unit                 1) Convergence usually take more time \n",
    "# 2) Robuts to outlier\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39ab9e4a-e762-4587-8a5b-cd907fd1d96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is\n",
    "# it more appropriate to use?\n",
    "# Lasso Regularization ( L1 regularzation ) It is used for feature slection process , those feature is not more correlated \n",
    "# with output feature then there slope is small and in the convergence algothm if we increase the value of lambda then it will \n",
    "# converge the slope zero . \n",
    "# cost funcion=y=Σ(yi-ŷi)^2/n+ lambda * Σ|slope|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f5b94a0-6a5a-4c11-9401-4aca753d7abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an\n",
    "# # example to illustrate.\n",
    "# 1) Ridge Regularization (L2 regularization ) help to prevent overfiiting in machine leraning . We modifed the cost function \n",
    "# such that cost func =Σ(yi-ŷi)^2/n+ lambda * Σ( slope)^2\n",
    "# it will reduce the error on the new dataset but error will never be zero on the new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "442f71da-5b76-423b-b94e-8083d0738053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best\n",
    "# choice for regression analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "266ae8bb-16b0-40a5-bece-5ecff666d063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) It is used when there is linear relationship between the variables it is not used with non-linearity relationship\n",
    "# 2) Feature selection \n",
    "# 3) Outliers and influential observations\n",
    "# While regularized linear models offer valuable benefits,\n",
    "# they also have limitations that make them not always the best choice for regression analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00341158-7ebd-4455-b640-cac938da4073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9. You are comparing the performance of two regression models using different evaluation metrics.\n",
    "# Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
    "# performer, and why? Are there any limitations to your choice of metric?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87f6fa81-db59-4d33-9745-a9a9f3b6bf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE if the dataset have outlier and error is high in this so it will more shift toward the outlier where MAE have less error \n",
    "# it will shift equally toward outlier so we can say that \n",
    "# If the emphasis is on capturing larger errors more significantly, then Model A with an RMSE of 10 would be considered \n",
    "# the better performer.\n",
    "\n",
    "# However, if the focus is on the average magnitude of errors, regardless of their size, then Model B with an MAE of 8 \n",
    "# would be considered the better performer.\n",
    "\n",
    "# Ultimately, the choice of the better model depends on the specific objectives and requirements of the problem, and \n",
    "# it is important to consider the limitations and context of the evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dac0adee-d34f-4d8b-9827-c1517df05d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q10. You are comparing the performance of two regularized linear models using different types of\n",
    "# regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
    "# uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\n",
    "# better performer, and why? Are there any trade-offs or limitations to your choice of regularization\n",
    "# method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ad137c8-6b80-4411-86f3-0f1dbe4de81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Here both are different Rdige is used for reducing overfitting and lasso is used for feature selection.\n",
    "# # In Ridge we have never zero error so, Model A uses Ridge regularization with a regularization parameter of 0.1\n",
    "# # so we can choose this model when we have reduce overfitting and while Model B\n",
    "# # # uses Lasso regularization with a regularization parameter of 0.5. So in this if we have to do feature selection then it will perform better here .\n",
    "\n",
    "# Ridge regularization does not perform feature selection, meaning it keeps all features in the model, even if they have minimal impact\n",
    "# Lasso regularization, on the other hand, can eliminate irrelevant features by driving their coefficients to zero.\n",
    "# Lasso regularization tends to perform better in situations where the number of features is larger than the number of samples, \n",
    "# as it can effectively reduce the feature space. Ridge regularization does not offer this feature selection capability.\n",
    "# The choice of the regularization parameter (lambda ) is crucial. If the parameter is too high, both regularization methods can \n",
    "# overly shrink the coefficients, leading to underfitting. If it is too low, the models may not effectively prevent overfitting.\n",
    "# Both Ridge and Lasso regularization assume linearity between the features and the target variable. If the relationship is highly \n",
    "# non-linear, other models or regularization techniques might be more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a57409-fa96-442e-abd0-36fee39cc5d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
